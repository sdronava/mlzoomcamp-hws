{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9889db55-a8aa-435c-b326-b4372eb4b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a02015-6a68-4b2a-b775-72681d86c330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-23 19:03:50--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10472 (10K) [text/plain]\n",
      "Saving to: ‘imagenet_classes.txt’\n",
      "\n",
      "imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2025-11-23 19:03:50 (4.72 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt -O imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2e2d4e-37f1-4200-a183-93e7e13a7a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m./data\u001b[0m\n",
      "├── \u001b[1;36mtest\u001b[0m\n",
      "│   ├── \u001b[1;36mcurly\u001b[0m\n",
      "│   └── \u001b[1;36mstraight\u001b[0m\n",
      "└── \u001b[1;36mtrain\u001b[0m\n",
      "    ├── \u001b[1;36mcurly\u001b[0m\n",
      "    └── \u001b[1;36mstraight\u001b[0m\n",
      "\n",
      "7 directories\n"
     ]
    }
   ],
   "source": [
    "!tree -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ba135-30bc-477f-a547-39e39334c7db",
   "metadata": {},
   "source": [
    "Reproducibility\n",
    "\n",
    "Use Set Randomm Seed Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77eb90f8-fbaf-4164-b3f3-49f302df6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Library\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26405dbd-29e4-42ac-b1fd-0f52b8f85238",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1cbf66-6bbe-4045-a7f6-b4ef7bdab93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5_classes_from(indices):\n",
    "    with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "        categories = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "    # Get top 5 predictions\n",
    "    top5_indices = indices[0, :5].tolist()\n",
    "    top5_classes = [categories[i] for i in top5_indices]\n",
    "    \n",
    "    print(\"Top 5 predictions:\")\n",
    "    for i, class_name in enumerate(top5_classes):\n",
    "        print(f\"{i+1}: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570a2034-38d4-4f79-b512-57802fbbea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "1: wig\n",
      "2: abaya\n",
      "3: chain mail\n",
      "4: wool\n",
      "5: stole\n",
      "Top 5 predictions:\n",
      "1: wig\n",
      "2: neck brace\n",
      "3: suit\n",
      "4: brassiere\n",
      "5: lab coat\n"
     ]
    }
   ],
   "source": [
    "#PIL Load Images Example (Not on Colab)\n",
    "preprocess_example = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "curly_img = \"data/train/curly/images78.jpg\"\n",
    "straight_img = \"data/train/straight/s8.jpg\"\n",
    "\n",
    "#Use predefined model and predict without any further training.\n",
    "# Load pre-trained model\n",
    "model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "model.eval()\n",
    "\n",
    "for img_path in [curly_img, straight_img]:\n",
    "    img = Image.open(img_path)\n",
    "    # Resize to target size\n",
    "    img = preprocess_example(img)\n",
    "\n",
    "    batch_t = torch.unsqueeze(img, 0)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_t)\n",
    "\n",
    "    _, sorted_indices = torch.sort(output, descending=True)\n",
    "    top5_classes_from(sorted_indices)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74406ae9-2055-4c6a-bce2-ba957ac4a24b",
   "metadata": {},
   "source": [
    "# Prepare Training and Validation Datasets\n",
    "\n",
    "- Resize\n",
    "- Permute\n",
    "- Batch Size\n",
    "- Shuffle or Not\n",
    "- Normalize\n",
    "- Augment (optional regularization control to be applied only to training datasets\n",
    "\n",
    "# Homework - Model \n",
    "You need to develop the model with following structure:\n",
    "\n",
    "The shape for input should be (3, 200, 200) (channels first format in PyTorch)\n",
    "\n",
    "Next, create a convolutional layer (nn.Conv2d):\n",
    "Use 32 filters (output channels)\n",
    "Kernel size should be (3, 3) (that's the size of the filter)\n",
    "Use 'relu' as activation\n",
    "Reduce the size of the feature map with max pooling (nn.MaxPool2d)\n",
    "Set the pooling size to (2, 2)\n",
    "Turn the multi-dimensional result into vectors using flatten or view\n",
    "Next, add a nn.Linear layer with 64 neurons and 'relu' activation\n",
    "Finally, create the nn.Linear layer with 1 neuron - this will be the output\n",
    "The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "As optimizer use torch.optim.SGD with the following parameters:\n",
    "\n",
    "torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b08e1a22-d797-44af-a035-1244ea997bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BinaryCNN(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 200, 200)):\n",
    "        super(BinaryCNN, self).__init__()\n",
    "\n",
    "        C, H, W = input_shape\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=C,\n",
    "            out_channels=32,\n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Activation modules\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        #Some ChatGPT magic to calculate this\n",
    "        dummy = torch.zeros(1, C, H, W)\n",
    "        dummy_out = self.pool(self.relu(self.conv1(dummy)))\n",
    "        self.flattened_size = dummy_out.numel()\n",
    "        print(\"flattened_size:\", self.flattened_size)\n",
    "\n",
    "        # Flatten and Dense\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)  # output neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)  # binary output probability\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76bcba4-3035-4e77-8542-e8bae683315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_size: 313632\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 198, 198]             896\n",
      "              ReLU-2         [-1, 32, 198, 198]               0\n",
      "         MaxPool2d-3           [-1, 32, 99, 99]               0\n",
      "            Linear-4                   [-1, 64]      20,072,512\n",
      "              ReLU-5                   [-1, 64]               0\n",
      "            Linear-6                    [-1, 1]              65\n",
      "           Sigmoid-7                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 20,073,473\n",
      "Trainable params: 20,073,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.46\n",
      "Forward/backward pass size (MB): 21.54\n",
      "Params size (MB): 76.57\n",
      "Estimated Total Size (MB): 98.57\n",
      "----------------------------------------------------------------\n",
      "Total parameters: 20073473\n"
     ]
    }
   ],
   "source": [
    "# ---------- Example Usage ----------\n",
    "\n",
    "model = BinaryCNN(input_shape=(3, 200, 200))\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 200, 200))\n",
    "\n",
    "# Option 2: Manual counting\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97c3a3-8d22-491b-8831-8bc669a651e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662ce1ed-fcf1-4c03-89b5-b128d96e8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(data_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for label_name in self.classes:\n",
    "            label_dir = os.path.join(data_dir, label_name)\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
    "                self.labels.append(self.class_to_idx[label_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dd30f38-0dcd-4a52-bb53-850e4426aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard-code everything ...\n",
    "def make_model():\n",
    "    model = BinaryCNN(input_shape=(3, 200, 200))\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb14ad6-6c7b-4cac-a64c-4817ee4c7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 200\n",
    "\n",
    "# ImageNet normalization values\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std\n",
    "    ) \n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std\n",
    "    ) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90819368-25e6-4712-a23f-2491daf70c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BinaryDataset(\n",
    "    data_dir='./data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "validation_dataset = BinaryDataset(\n",
    "    data_dir='./data/test',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "#test\n",
    "test_image, test_label = train_dataset.__getitem__(0)\n",
    "print(test_image.shape)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0293809-154d-41e6-a5c1-42ca726a5517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_size: 313632\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 198, 198]             896\n",
      "              ReLU-2         [-1, 32, 198, 198]               0\n",
      "         MaxPool2d-3           [-1, 32, 99, 99]               0\n",
      "            Linear-4                   [-1, 64]      20,072,512\n",
      "              ReLU-5                   [-1, 64]               0\n",
      "            Linear-6                    [-1, 1]              65\n",
      "           Sigmoid-7                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 20,073,473\n",
      "Trainable params: 20,073,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.46\n",
      "Forward/backward pass size (MB): 21.54\n",
      "Params size (MB): 76.57\n",
      "Estimated Total Size (MB): 98.57\n",
      "----------------------------------------------------------------\n",
      "Total parameters: 20073473\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Using torchsummary (install with: pip install torchsummary)\n",
    "model, optimizer, criterion = make_model()\n",
    "summary(model, input_size=(3, 200, 200))\n",
    "\n",
    "# Option 2: Manual counting\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aa0089f-a61f-4d12-b621-72362cc99a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6963, Acc: 0.4869, Val Loss: 0.6926, Val Acc: 0.4876\n",
      "Epoch 2/10, Loss: 0.6905, Acc: 0.4869, Val Loss: 0.6848, Val Acc: 0.4876\n",
      "Epoch 3/10, Loss: 0.6923, Acc: 0.4869, Val Loss: 0.6920, Val Acc: 0.4876\n",
      "Epoch 4/10, Loss: 0.6858, Acc: 0.4869, Val Loss: 0.6765, Val Acc: 0.4876\n",
      "Epoch 5/10, Loss: 0.6559, Acc: 0.4869, Val Loss: 0.6611, Val Acc: 0.4876\n",
      "Epoch 6/10, Loss: 0.6928, Acc: 0.5993, Val Loss: 0.6919, Val Acc: 0.5323\n",
      "Epoch 7/10, Loss: 0.6808, Acc: 0.5693, Val Loss: 0.6849, Val Acc: 0.4925\n",
      "Epoch 8/10, Loss: 0.6514, Acc: 0.5019, Val Loss: 0.6628, Val Acc: 0.4925\n",
      "Epoch 9/10, Loss: 0.6434, Acc: 0.4981, Val Loss: 0.6607, Val Acc: 0.4925\n",
      "Epoch 10/10, Loss: 0.6370, Acc: 0.4969, Val Loss: 0.6656, Val Acc: 0.4876\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40655a6a-19b9-4cf0-ab77-a2237e0121c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.48756218905472637)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "706f1cfc-78f6-44fd-85f8-dfe0f4217630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4918851435705368)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c1698d-e552-46f9-a53a-5b5dc9dbb15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.021832754604145015)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe0a89ca-3ef2-48b5-a012-cbc191f87ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.012907196471526222)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7d5b458-cf5e-4b0d-9f14-c8d847164332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple transforms - just resize and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.RandomRotation(50),\n",
    "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std\n",
    "    ) \n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std\n",
    "    ) \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = BinaryDataset(\n",
    "    data_dir='./data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "validation_dataset = BinaryDataset(\n",
    "    data_dir='./data/test',\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "#test\n",
    "test_image, test_label = train_dataset.__getitem__(0)\n",
    "print(test_image.shape)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a28d7f3-9509-4c8b-b483-4298d9651e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6593, Acc: 0.4881, Val Loss: 0.6506, Val Acc: 0.5075\n",
      "Epoch 2/10, Loss: 0.6503, Acc: 0.4906, Val Loss: 0.6612, Val Acc: 0.4925\n",
      "Epoch 3/10, Loss: 0.6571, Acc: 0.4906, Val Loss: 0.6481, Val Acc: 0.5075\n",
      "Epoch 4/10, Loss: 0.6519, Acc: 0.4956, Val Loss: 0.6474, Val Acc: 0.5124\n",
      "Epoch 5/10, Loss: 0.6467, Acc: 0.4969, Val Loss: 0.6535, Val Acc: 0.5025\n",
      "Epoch 6/10, Loss: 0.6624, Acc: 0.4931, Val Loss: 0.6477, Val Acc: 0.5075\n",
      "Epoch 7/10, Loss: 0.6575, Acc: 0.4881, Val Loss: 0.6643, Val Acc: 0.5373\n",
      "Epoch 8/10, Loss: 0.6557, Acc: 0.4956, Val Loss: 0.6460, Val Acc: 0.5174\n",
      "Epoch 9/10, Loss: 0.6448, Acc: 0.4931, Val Loss: 0.6456, Val Acc: 0.5174\n",
      "Epoch 10/10, Loss: 0.6470, Acc: 0.4969, Val Loss: 0.6483, Val Acc: 0.5025\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebb8343c-1543-4bef-a266-ee26869628a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6512589687879999)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a269f1a-7ddd-494f-bee4-1dd7b0f6745c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5074626865671642,\n",
       " 0.5373134328358209,\n",
       " 0.5174129353233831,\n",
       " 0.5174129353233831,\n",
       " 0.5024875621890548]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['val_acc'][5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da9f561-208c-4009-97d3-a22eaff80db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5164179104477611)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(history['val_acc'][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df764f2c-aab2-4c8f-b43b-7f620e80b4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
